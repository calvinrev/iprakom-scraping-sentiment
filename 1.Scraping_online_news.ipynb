{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fabefde",
   "metadata": {},
   "source": [
    "# Requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba136f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## jalankan ini dahulu sebelum running\n",
    "%pip install -qq -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1cc8f7",
   "metadata": {},
   "source": [
    "# A. Case: Detik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9df462",
   "metadata": {},
   "outputs": [],
   "source": [
    "## global\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup #scraping web statis\n",
    "import time\n",
    "from tqdm import tqdm #info progress\n",
    "from urllib.parse import quote_plus, quote #parsing string \"spasi\" menjadi \"%20\" dan \"+\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac2ca3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check if there is a response, if it's 200, we are good to go\n",
    "s = requests.Session()\n",
    "url = 'https://www.detik.com'\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36'\n",
    "}\n",
    "response = s.get(url, headers=headers, timeout=60)\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb169f7d",
   "metadata": {},
   "source": [
    "### A1. Crawling URLs dengan memasukkan keyword/query\n",
    "```https://www.detik.com/search/searchall?query=makan%20bergizi%20gratis&page=5&result_type=relevansi``` -> pakai modulo 20\n",
    "\n",
    "```https://www.detik.com/search/searchall?query=makan+bergizi+gratis&page=5&result_type=relevansi``` -> pakai +\n",
    "\n",
    "\n",
    "```https://www.detik.com/search/searchall?query={kata_kunci}&page={halaman}&result_type=relevansi```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34fd750",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f\"https://www.detik.com/search/searchall?query=makan%20bergizi&page=1&result_type=relevansi\"\n",
    "response = requests.get(url, headers=headers, timeout=60)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45bc7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## fungsi untuk scraping artikel berdasarkan keyword yang diinputkan\n",
    "def scrape_detik_byquery(kata_kunci, halaman) -> pd.DataFrame:\n",
    "    # Parameter input\n",
    "    keyword = kata_kunci\n",
    "    max_pages = halaman\n",
    "    results = []\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    for page in tqdm(range(1, max_pages + 1)):\n",
    "        encoded_query = quote(keyword)\n",
    "        url = f\"https://www.detik.com/search/searchall?query={encoded_query}&page={page}&result_type=relevansi\"\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=60)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            time.sleep(1)\n",
    "\n",
    "            beritas = soup.find_all('div', class_=\"media__text\")\n",
    "            for berita in beritas:\n",
    "                try:\n",
    "                    a = berita.find('a')\n",
    "                    d = berita.find('div', class_=\"media__desc\")\n",
    "                    t = berita.find('div', class_=\"media__date\")\n",
    "                    k = berita.find('h2', class_=\"media__subtitle\")\n",
    "\n",
    "                    results.append({\n",
    "                        \"judul\": a.text.strip() if a else np.nan,\n",
    "                        \"link\": a.get('href') if a else np.nan,\n",
    "                        \"desc\": d.text.strip() if d else np.nan,\n",
    "                        \"tanggal\": t.text.strip() if t else np.nan,\n",
    "                        \"kategori\": k.text.strip() if k else np.nan,\n",
    "                        \"keyword\": keyword\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(\"Skip 1 berita:\", e)\n",
    "        except Exception as e:\n",
    "            print(f\"Skip page {page}:\", e)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    print(\"Selesai. Total berita:\", len(df))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eba8467",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = scrape_detik_byquery(\"makan bergizi gratis\", 3)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fd091f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7361e190",
   "metadata": {},
   "source": [
    "\"Dedi Mulyadi kirim siswa nakal\"\n",
    "\n",
    "\"barak militer siswa Jawa Barat\"\n",
    "\n",
    "\"pelatihan militer siswa nakal\"\n",
    "\n",
    "\"pendidikan karakter Dedi Mulyadi\"\n",
    "\n",
    "\"kontroversi siswa ke militer\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420c459a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## looping list keyword\n",
    "\n",
    "# Daftar keyword yang ingin di-scrape\n",
    "list_keyword = [\n",
    "    \"Dedi Mulyadi kirim siswa nakal\",\n",
    "    \"barak militer siswa Jawa Barat\",\n",
    "    \"pelatihan militer siswa nakal\",\n",
    "    \"pendidikan karakter Dedi Mulyadi\",\n",
    "    \"kontroversi siswa ke militer\"\n",
    "]\n",
    "\n",
    "# Set jumlah halaman yang ingin di-scrape per keyword\n",
    "halaman = 5\n",
    "\n",
    "# List untuk menyimpan semua DataFrame\n",
    "dfs = []\n",
    "\n",
    "for keyword in list_keyword:\n",
    "    print(f\"Scraping untuk keyword: {keyword}\")\n",
    "    df_keyword = scrape_detik_byquery(keyword, halaman)\n",
    "    dfs.append(df_keyword)\n",
    "\n",
    "# Gabungkan semua hasil\n",
    "df = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2424ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ceae74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meremove duplicate berita\n",
    "df_final = df.drop_duplicates(subset=['judul', 'link'], keep='first')\n",
    "df_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f1c75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv(\"./files/detik_links.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb763d1",
   "metadata": {},
   "source": [
    "### A2. Scraping satu artikel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2fc4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## read data\n",
    "df_read = pd.read_csv(\"./files/detik_links.csv\", encoding='utf-8-sig')\n",
    "df_read.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9544e247",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07a419c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['keyword'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f63daf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://sport.detik.com/sepakbola/liga-indonesia/d-7932517/kdm-soal-bobotoh-rusak-rumput-gbla-pidana-atau-barak-militer\", headers=headers, timeout=60)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f41345d",
   "metadata": {},
   "outputs": [],
   "source": [
    "body_div = soup.find('div', class_='detail__body-text itp_bodycontent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90584c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_paragraphs = body_div.find_all('p') if body_div else np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac345b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Temukan semua <p class=\"para_caption\"> dalam div.parallaxindetail.scrollpage\n",
    "excluded_paragraphs = set()\n",
    "for para_section in body_div.find_all('div', class_='parallaxindetail scrollpage'):\n",
    "    for p in para_section.find_all('p', class_='para_caption'):\n",
    "        excluded_paragraphs.add(p)\n",
    "\n",
    "# 3. Filter: ambil semua <p> dari body_div yang tidak ada dalam excluded_paragraphs\n",
    "final_paragraphs = [p for p in all_paragraphs if p not in excluded_paragraphs]\n",
    "combined_text = '\\n'.join(p.get_text(strip=True) for p in final_paragraphs)\n",
    "combined_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16075b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "## fungsi melakukan scraping satu artikel\n",
    "def scrape_detik_satu(url: str) -> pd.DataFrame:\n",
    "    response = requests.get(url, headers=headers, timeout=60)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Ambil teks tanggal\n",
    "    tanggal_utuh = soup.find('div', class_='detail__date')\n",
    "    if tanggal_utuh:\n",
    "        waktu = tanggal_utuh.get_text(strip=True)\n",
    "        try:\n",
    "            # Pisahkan jadi hari, tanggal, dan jam\n",
    "            hari, sisanya = waktu.split(',', 1)\n",
    "            sisanya = sisanya.strip()\n",
    "            parts = sisanya.rsplit(' ', 2)\n",
    "            tanggal_bersih = parts[0]\n",
    "            jam = f\"{parts[1]} {parts[2]}\"\n",
    "        except Exception:\n",
    "            hari = np.nan\n",
    "            tanggal_bersih = np.nan\n",
    "            jam = np.nan\n",
    "    else:\n",
    "        hari = np.nan\n",
    "        tanggal_bersih = np.nan\n",
    "        jam = np.nan\n",
    "\n",
    "    # Ambil kategori\n",
    "    kat_div = soup.find('div', class_='page__breadcrumb')\n",
    "    kategori = np.nan\n",
    "    sub_kategori = np.nan\n",
    "    if kat_div:\n",
    "        kategori = kat_div.find('a').get_text(strip=True)\n",
    "        a_tag = kat_div.find('a', attrs={'dtr-ttl': True})\n",
    "        if a_tag:\n",
    "            sub_kategori = a_tag.get('dtr-ttl')\n",
    "\n",
    "    #Ambil Isi\n",
    "    # 1. Ambil semua <p> dalam div.detail__body-text.itp_bodycontent\n",
    "    body_div = soup.find('div', class_='detail__body-text itp_bodycontent')\n",
    "    if body_div is None:\n",
    "        # Kalau tidak ketemu, langsung ambil semua <p> di halaman\n",
    "        final_paragraphs = soup.find_all('p')\n",
    "    else:\n",
    "        # Ambil semua <p> dalam div\n",
    "        all_paragraphs = body_div.find_all('p')\n",
    "\n",
    "        # Temukan semua <p class=\"para_caption\"> dalam div.parallaxindetail.scrollpage\n",
    "        excluded_paragraphs = set()\n",
    "        for para_section in body_div.find_all('div', class_='parallaxindetail scrollpage'):\n",
    "            for p in para_section.find_all('p', class_='para_caption'):\n",
    "                excluded_paragraphs.add(p)\n",
    "\n",
    "        # Filter: ambil semua <p> dari body_div yang tidak ada dalam excluded_paragraphs\n",
    "        final_paragraphs = [p for p in all_paragraphs if p not in excluded_paragraphs]\n",
    "\n",
    "    # Gabungkan hasil\n",
    "    final_isi = '\\n\\n'.join(p.get_text(strip=True) for p in final_paragraphs)\n",
    "    \n",
    "    hasil = {\n",
    "        \"judul\": soup.find('h1').get_text(strip=True) if soup.find('h1') else np.nan,\n",
    "        \"isi\": final_isi,\n",
    "        \"hari\": hari,\n",
    "        \"tanggal\": tanggal_bersih,\n",
    "        \"jam\": jam,\n",
    "        # \"kategori\": soup.find('div', class_=\"page__breadcrumb\").get_text(strip=True) if soup.find('div', class_=\"page__breadcrumb\") else np.nan,\n",
    "        \"kategori\": kategori,\n",
    "        \"sub_kategori\": sub_kategori,\n",
    "        \"link\": url\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame([hasil])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a292428",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read[\"link\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52786d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_detik_satu(df_read[\"link\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fcff88",
   "metadata": {},
   "source": [
    "### A3. Scraping artikel dari URLs yang telah diperoleh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2eae6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_detik_dari_csv(path_csv: str) -> pd.DataFrame:\n",
    "    # Membaca CSV hasil scraping link\n",
    "    df_links = pd.read_csv(path_csv)\n",
    "\n",
    "    # Memastikan kolom 'link' ada\n",
    "    if 'link' not in df_links.columns:\n",
    "        raise ValueError(\"CSV tidak mengandung kolom 'link'.\")\n",
    "\n",
    "    hasil_semua = []\n",
    "    for i,row in tqdm(df_links.iterrows(), total=len(df_links)): #tuple (i,series)\n",
    "        url = row['link']\n",
    "        df_artikel = scrape_detik_satu(url) #memanggil dan menjalankan fungsi scrape_detik satu artikel\n",
    "        keyword = row['keyword']\n",
    "        if df_artikel is not None:\n",
    "            df_artikel['keyword'] = keyword\n",
    "            hasil_semua.append(df_artikel)\n",
    "\n",
    "    # Gabungkan semua DataFrame\n",
    "    if hasil_semua:\n",
    "        df_final = pd.concat(hasil_semua, ignore_index=True)\n",
    "        df_final.to_csv('./files/detik_semua_artikel_query.csv', index=False, encoding='utf-8-sig') #disesuaikan dengan path teman2\n",
    "        print(\"Selesai menyimpan semua artikel.\")\n",
    "        return df_final\n",
    "    else:\n",
    "        print(\"Tidak ada artikel yang berhasil di-scrape.\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272c5edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = scrape_detik_dari_csv('./files/detik_links.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3f4381",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9aa2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f488cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.to_csv('./files/detik_semua_artikel_query_clean.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb08474d",
   "metadata": {},
   "source": [
    "### A4. Detik.com menyediakan sitemap xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e5593a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ambil sitemap (allowed dari detik.com/robots.txt)\n",
    "url_sitemap = 'https://www.detik.com/sitemap.xml'\n",
    "response = requests.get(url_sitemap)\n",
    "soup = BeautifulSoup(response.content, 'xml')  # parsing sebagai XML\n",
    "\n",
    "# Ambil semua URL sitemap yang disediakan\n",
    "sitemap_urls = [loc.text for loc in soup.find_all('loc')]\n",
    "print(\"Contoh:\", sitemap_urls[0])\n",
    "print(\"Total sitemap url:\", len(sitemap_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ff0298",
   "metadata": {},
   "outputs": [],
   "source": [
    "## cek satu sitemap ada berapa artikel\n",
    "sitemap_berita = sitemap_urls[0] \n",
    "resp = requests.get(sitemap_berita)\n",
    "soup2 = BeautifulSoup(resp.content, 'xml')\n",
    "artikel_urls = [loc.text.strip() for loc in soup2.find_all('loc')]\n",
    "print(\"Total artikel:\", len(artikel_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5277a8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "artikel_urls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea7cf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "berita = []\n",
    "for url in artikel_urls[:5]:  # batasi dulu misalnya 5\n",
    "    try:\n",
    "        hasil = scrape_detik_satu(url)\n",
    "        berita.append(hasil)\n",
    "    except Exception as e:\n",
    "        print(\"Gagal:\", url, e)\n",
    "\n",
    "df = pd.concat(berita, ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cebe6f",
   "metadata": {},
   "source": [
    "### A5. Scraping seluruh isi sitemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603b067e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fungsi ambil url sitemap dari sitemap utama\n",
    "def get_sitemap_urls(master_sitemap_url: str):\n",
    "    response = requests.get(master_sitemap_url, headers=headers, timeout=60)\n",
    "    soup = BeautifulSoup(response.content, 'xml')\n",
    "    sitemap_urls = [loc.get_text().strip() for loc in soup.find_all('loc')]\n",
    "    return sitemap_urls\n",
    "\n",
    "#fungsi ambil url article\n",
    "def get_article_urls(sitemap_url: str):\n",
    "    response = requests.get(sitemap_url, headers=headers, timeout=60)\n",
    "    soup = BeautifulSoup(response.content, 'xml')\n",
    "    article_urls = [loc.get_text().strip() for loc in soup.find_all('loc')]\n",
    "    return article_urls\n",
    "\n",
    "def scrape_detik(url: str):\n",
    "    response = requests.get(url, headers=headers, timeout=60)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Ambil teks tanggal utuh\n",
    "    tanggal_utuh = soup.find('div', class_='detail__date')\n",
    "    if tanggal_utuh:\n",
    "        waktu = tanggal_utuh.get_text(strip=True)\n",
    "        try:\n",
    "            hari, sisanya = waktu.split(',', 1)\n",
    "            sisanya = sisanya.strip()\n",
    "            parts = sisanya.rsplit(' ', 2)\n",
    "            tanggal_bersih = parts[0]\n",
    "            jam = f\"{parts[1]} {parts[2]}\"\n",
    "        except Exception:\n",
    "            hari = np.nan\n",
    "            tanggal_bersih = np.nan\n",
    "            jam = np.nan\n",
    "    else:\n",
    "        hari = np.nan\n",
    "        tanggal_bersih = np.nan\n",
    "        jam = np.nan\n",
    "\n",
    "    hasil = {\n",
    "        \"judul\": soup.find('h1').get_text(strip=True) if soup.find('h1') else np.nan,\n",
    "        \"isi\": \"\\n\\n\".join(p.get_text(strip=True) for p in soup.find_all('p')),\n",
    "        \"hari\": hari,\n",
    "        \"tanggal\": tanggal_bersih,\n",
    "        \"jam\": jam,\n",
    "        \"kategori\": soup.find('div', class_=\"page__breadcrumb\").get_text(strip=True) if soup.find('div', class_=\"page__breadcrumb\") else np.nan,\n",
    "        \"link\": url\n",
    "    }\n",
    "\n",
    "    return hasil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0dc0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all_from_sitemap(master_sitemap_url: str, max_sitemap=None, max_articles=None, output_csv: str=\"detik_semua_artikel_sitemap.csv\"):\n",
    "    \"\"\"\n",
    "    Robot scraping semua artikel dari sitemap utama Detik.\n",
    "\n",
    "    Args:\n",
    "        master_sitemap_url (str): URL sitemap utama.\n",
    "        max_sitemap (int, optional): Batasi jumlah sitemap yang di-scrape. None artinya semua.\n",
    "        max_articles (int, optional): Batasi jumlah artikel per sitemap. None artinya semua.\n",
    "        output_csv (str): Nama file output CSV.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame berisi hasil scraping artikel.\n",
    "    \"\"\"\n",
    "    all_articles = []\n",
    "\n",
    "    print(\"Mengambil daftar sitemap...\")\n",
    "    sitemap_urls = get_sitemap_urls(master_sitemap_url)\n",
    "\n",
    "    if max_sitemap:\n",
    "        sitemap_urls = sitemap_urls[:max_sitemap]\n",
    "\n",
    "    for sitemap in tqdm(sitemap_urls, desc=\"Sitemap\"):\n",
    "        try:\n",
    "            article_urls = get_article_urls(sitemap)\n",
    "            if max_articles:\n",
    "                article_urls = article_urls[:max_articles]\n",
    "\n",
    "            for url in tqdm(article_urls, desc=\"Artikel\", leave=False):\n",
    "                try:\n",
    "                    article = scrape_detik(url)\n",
    "                    all_articles.append(article)\n",
    "                    time.sleep(1)  # Hindari ban\n",
    "                except Exception as e:\n",
    "                    print(\"Gagal scraping artikel:\", url, e)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Gagal akses sitemap:\", sitemap, e)\n",
    "\n",
    "    # Simpan ke CSV\n",
    "    df = pd.DataFrame(all_articles)\n",
    "    df.to_csv(output_csv, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Selesai. Total artikel: {len(df)}. Hasil disimpan di {output_csv}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498c0044",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = scrape_all_from_sitemap('https://www.detik.com/sitemap.xml', max_sitemap=2, max_articles=10)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ea0e2c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78bd274c",
   "metadata": {},
   "source": [
    "# B. Case: Tempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecadb752",
   "metadata": {},
   "outputs": [],
   "source": [
    "## global\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup #scraping web statis\n",
    "import time\n",
    "from tqdm import tqdm #info progress\n",
    "\n",
    "## selenium, scraping web dinamis\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23bd6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check if there is a response, if it's 200, we are good to go\n",
    "s = requests.Session()\n",
    "url = 'https://www.tempo.co/search?q=makan+bergizi+gratis&page=1'\n",
    "response = s.get(url)\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1679f977",
   "metadata": {},
   "source": [
    "### B1. Crawling URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8954ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Fungsi melakukan scraping URL Tempo, masukkan string keyword dan max halaman\n",
    "def scrape_tempo_search_selenium(kata_kunci: str, halaman: int) -> pd.DataFrame: \n",
    "    \"\"\"\n",
    "    Robot crawling url yang diinginkan berdasarkan kata kunci yang user input.\n",
    "\n",
    "    Args:\n",
    "        kata_kunci (str): Query yang ingin dimasukkan.\n",
    "        halaman (int): Batasi jumlah halaman yang di-scrape.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame berisi hasil crawling url.\n",
    "    \"\"\"\n",
    "    # Set User-Agent\n",
    "    user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36'\n",
    "    opts = Options()\n",
    "    opts.add_argument(f\"user-agent={user_agent}\")\n",
    "    opts.add_argument(\"--headless\")\n",
    "\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=opts)\n",
    "        \n",
    "    # Parameter input\n",
    "    keyword = kata_kunci #contoh: \"makan bergizi gratis\"\n",
    "    max_pages = halaman #contoh: 2\n",
    "    results = []\n",
    "\n",
    "    # Loop halaman\n",
    "    for page in tqdm(range(1, max_pages + 1)):\n",
    "        # print(f\"Scraping page {page}...\")\n",
    "\n",
    "        # Format URL pencarian\n",
    "        encoded_query = quote_plus(keyword)\n",
    "        url = f\"https://www.tempo.co/search?q={encoded_query}&page={page}\"\n",
    "        \n",
    "        driver.get(url)\n",
    "        time.sleep(10)\n",
    "\n",
    "        try:\n",
    "            container = driver.find_element(\"css selector\", \"div.flex.flex-col.divide-y.divide-neutral-500\")\n",
    "            beritas = container.find_elements(\"css selector\", \"figure.flex.flex-row.gap-3.py-4.container.lg\\\\:mx-0.lg\\\\:px-0\")\n",
    "            for berita in beritas:\n",
    "                try:\n",
    "                    a = berita.find_element(\"tag name\", \"a\")\n",
    "                    p = berita.find_element(\"tag name\", \"p\")\n",
    "                    results.append({\n",
    "                        \"judul\": p.text,\n",
    "                        \"link\": a.get_attribute(\"href\"),\n",
    "                        \"keyword\": keyword\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(\"Skip 1 berita:\", e)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Skip page:\", e)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    # Simpan ke DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    print(\"Selesai. Total berita:\", len(df))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b6ac67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#max pages yang bisa diakses publik hanya 100 pages untuk tiap keyword (pengecekan manual)\n",
    "scrape_tempo_search_selenium(\"makan bergizi gratis\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4eea15",
   "metadata": {},
   "outputs": [],
   "source": [
    "## looping list keyword\n",
    "\n",
    "# Daftar keyword yang ingin di-scrape\n",
    "list_keyword = [\n",
    "    \"makan bergizi gratis\",\n",
    "    \"efisiensi anggaran\",\n",
    "    \"CPNS 2025\",\n",
    "    \"kemiskinan world bank\"\n",
    "]\n",
    "\n",
    "# Set jumlah halaman yang ingin di-scrape per keyword\n",
    "halaman = 2\n",
    "\n",
    "# List untuk menyimpan semua DataFrame\n",
    "dfs = []\n",
    "\n",
    "for keyword in list_keyword:\n",
    "    print(f\"Scraping untuk keyword: {keyword}\")\n",
    "    df_keyword = scrape_tempo_search_selenium(keyword, halaman)\n",
    "    dfs.append(df_keyword)\n",
    "\n",
    "# Gabungkan semua hasil\n",
    "df_final = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2443d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv(\"./files/tempo_links.csv\", index=False, encoding='utf-8-sig')\n",
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64e16de",
   "metadata": {},
   "source": [
    "### B2. Scraping satu artikel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d79631d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Fungsi melakukan scraping data 1 halaman Tempo, masukkan string url\n",
    "def scrape_tempo(url: str) -> pd.DataFrame: \n",
    "    ## Inisiasi dictionary hasil\n",
    "    hasil = {}\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=60)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        ##Judul\n",
    "        judul = soup.find('h1', class_='text-[26px] font-bold leading-[122%] text-neutral-1200')\n",
    "        hasil['judul'] = judul.get_text(strip=True) if judul else np.nan\n",
    "\n",
    "        ## Sub judul\n",
    "        sub_judul = soup.find('div', class_='font-roboserif leading-[156%] text-neutral-1100')\n",
    "        hasil['sub_judul'] = sub_judul.get_text(strip=True) if sub_judul else np.nan\n",
    "\n",
    "        ## Isi berita\n",
    "        isi_paragraf = []\n",
    "        isi_berita = soup.find_all('div', id='content-wrapper', class_='max-lg:container xl')\n",
    "\n",
    "        for i in isi_berita:\n",
    "            paragraf = i.find_all('p')\n",
    "            for p in paragraf:\n",
    "                teks = p.get_text(strip=True)\n",
    "                if teks:  #menambahkan teks bila ada\n",
    "                    isi_paragraf.append(teks)\n",
    "        ringkasan = '\\n\\n'.join(isi_paragraf)\n",
    "        hasil['isi'] = ringkasan if ringkasan else np.nan\n",
    "\n",
    "        ## Tanggal & Jam publikasi\n",
    "        tanggal_publikasi = soup.find('p', class_='text-neutral-900 text-sm')\n",
    "        if tanggal_publikasi:\n",
    "            waktu = tanggal_publikasi.get_text(strip=True)\n",
    "            if '|' in waktu:\n",
    "                tanggal, jam = [part.strip() for part in waktu.split('|')]\n",
    "                hasil['tanggal'] = tanggal\n",
    "                hasil['jam'] = jam\n",
    "            else:\n",
    "                hasil['tanggal'] = waktu\n",
    "                hasil['jam'] = np.nan\n",
    "        else:\n",
    "            hasil['tanggal'] = np.nan\n",
    "            hasil['jam'] = np.nan\n",
    "\n",
    "        ## Kategori\n",
    "        kategori = soup.find('span', class_='text-sm font-medium text-primary-main')\n",
    "        hasil['kategori'] = kategori.get_text(strip=True) if kategori else np.nan\n",
    "\n",
    "        ## Link\n",
    "        hasil['link'] = url \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Terjadi kesalahan saat scraping: {e}\")\n",
    "        return None\n",
    "\n",
    "    ## Kembalikan juga sebagai DataFrame\n",
    "    df = pd.DataFrame([hasil])\n",
    "    # print('selesai scraping')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792d76e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.tempo.co/ekonomi/potensi-masalah-dari-rencana-pemerintah-ubah-lapas-jadi-perumahan-1533913'\n",
    "df_hasil = scrape_tempo(url)\n",
    "df_hasil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64e6359",
   "metadata": {},
   "source": [
    "### B3. Scraping artikel dalam URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b16d691",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Fungsi melakukan scraping data dari hasil crawling URL Tempo, masukkan csv\n",
    "def scrape_tempo_dari_csv(path_csv: str) -> pd.DataFrame:\n",
    "    # Membaca CSV hasil scraping link\n",
    "    df_links = pd.read_csv(path_csv)\n",
    "\n",
    "    # Memastikan kolom 'link' ada\n",
    "    if 'link' not in df_links.columns:\n",
    "        raise ValueError(\"CSV tidak mengandung kolom 'link'.\")\n",
    "\n",
    "    hasil_semua = []\n",
    "    for i, row in tqdm(df_links.iterrows(), total=len(df_links)):\n",
    "        url = row['link']\n",
    "        df_artikel = scrape_tempo(url) #memanggil dan menjalankan fungsi scrape_tempo satu artikel\n",
    "        keyword = row['keyword']\n",
    "        if df_artikel is not None:\n",
    "            df_artikel['keyword'] = keyword\n",
    "            hasil_semua.append(df_artikel)\n",
    "\n",
    "    # Gabungkan semua DataFrame\n",
    "    if hasil_semua:\n",
    "        df_final = pd.concat(hasil_semua, ignore_index=True)\n",
    "        df_final.to_csv('./files/tempo_semua_artikel.csv', index=False, encoding='utf-8-sig')\n",
    "        print(\"Selesai menyimpan semua artikel.\")\n",
    "        return df_final\n",
    "    else:\n",
    "        print(\"Tidak ada artikel yang berhasil di-scrape.\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de1ea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = scrape_tempo_dari_csv(\"./files/tempo_links.csv\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8879e2c",
   "metadata": {},
   "source": [
    "# Next: Analisis Sentimen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca43466",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
